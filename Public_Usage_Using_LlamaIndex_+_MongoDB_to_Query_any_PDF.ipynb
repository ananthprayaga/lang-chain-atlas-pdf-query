{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "489809b2",
      "metadata": {
        "id": "489809b2"
      },
      "outputs": [],
      "source": [
        "# !pip3 install langchain\n",
        "# !pip3 install llama-index==0.6.0\n",
        "# !pip3 install pymongo\n",
        "# !pip3 install nltk\n",
        "# !pip3 install Pillow\n",
        "# !pip3 install python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a54d1c43-4b7f-4917-939f-a964f6f3dafc",
      "metadata": {
        "id": "a54d1c43-4b7f-4917-939f-a964f6f3dafc",
        "scrolled": true,
        "tags": []
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa67fa07-1395-4aab-a356-72bdb302f6b2",
      "metadata": {
        "id": "fa67fa07-1395-4aab-a356-72bdb302f6b2",
        "scrolled": true,
        "tags": []
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import sys\n",
        "\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
        "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d12d766-3ca8-4012-9da2-248be80bb6ab",
      "metadata": {
        "id": "1d12d766-3ca8-4012-9da2-248be80bb6ab",
        "scrolled": true,
        "tags": []
      },
      "outputs": [],
      "source": [
        "from llama_index import (\n",
        "    LLMPredictor,\n",
        "    GPTVectorStoreIndex, \n",
        "    GPTListIndex, \n",
        "    GPTSimpleKeywordTableIndex,\n",
        "    download_loader\n",
        ")\n",
        "\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from llama_index.response.notebook_utils import display_response"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install PyPDF"
      ],
      "metadata": {
        "id": "hqITFS5S4UbI"
      },
      "id": "hqITFS5S4UbI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "tinxuHz_05Vu",
      "metadata": {
        "id": "tinxuHz_05Vu"
      },
      "source": [
        "### INTRO\n",
        "\n",
        "At a basic level, LlamaIndex takes your documents and breaks them into chunks called nodes.\n",
        "\n",
        "Workflow:\n",
        "1) Connect the private knowledge sources using LlamaIndex connectors. \n",
        "2) Load in the Documents. A ‘LlamaIndex Document’ represents a lightweight container around the data source. \n",
        "3) Parse the ‘LlamaIndex Documents’ objects into ‘LlamaIndex Nodes’ objects. Nodes represent “chunks” of source ‘LlamaIndex Documents’ (ex., a text chunk). These node objects can be persisted in a MongoDB collection.\n",
        "4) Construct ‘LlamaIndex Index’ from ‘LlamaIndex Nodes’. There are various kinds of indexes in LlamaIndex, like “List Index” (which stores Nodes as a Sequential chain) and “Vector Store Index” (this stores each node and a corresponding embedding in a vector store). Depending on the type of Index, these indexes can be persisted into a MongoDB collection or a Vector Database.\n",
        "5) Finally, query the Index. The query is parsed at this step; relevant Nodes are retrieved through indexes and provided as input to the “Large Language Model” (LLM). Different types of queries can use different indexes.\n",
        "\n",
        "\n",
        "Use of Indexes:\n",
        "For summarization, you have two options: GPTListIndex or GPTVectorStoreIndex with response_mode=\"tree_summarize\". The distinction lies in the approach taken to generate the summary. A list index utilizes every node in the index to create the summary, while a vector index utilizes only the top k nodes to generate a summary.\n",
        "\n",
        "For Q&A, GPTVectorStoreIndex can be used. During the query, the system fetches the top k most relevant nodes based on your query text. These nodes are then used as context to synthesize an answer using the LLM."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "977352b8",
      "metadata": {
        "id": "977352b8"
      },
      "source": [
        "### Initialize OpenAI and MongoDB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8758826e",
      "metadata": {
        "id": "8758826e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "outputId": "45bf8c41-d91e-4bc4-ffa6-fd3a5086d9ec"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e9b81bfe498b>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdotenv\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dotenv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mload_dotenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dotenv'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import os\n",
        "\n",
        "# Set environment variable\n",
        "os.environ['OPENAI_API_KEY'] = 'Your-API-KEY-Here'\n",
        "\n",
        "# Access environment variable\n",
        "print(os.environ['OPENAI_API_KEY'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6dd9d5f-a601-4097-894e-fe98a0c35a5b",
      "metadata": {
        "id": "f6dd9d5f-a601-4097-894e-fe98a0c35a5b"
      },
      "source": [
        "#### Load Documents"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_hub.file.base import SimpleDirectoryReader\n",
        "\n",
        "loader = SimpleDirectoryReader('Your file path here')\n",
        "documents = loader.load_data()# !pip install llama_hub"
      ],
      "metadata": {
        "id": "-KpUD-xl4eeq"
      },
      "id": "-KpUD-xl4eeq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#documents"
      ],
      "metadata": {
        "id": "_0UDPecfB1Ri"
      },
      "id": "_0UDPecfB1Ri",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d4c9479-8bfd-4f2b-b4d3-bd4d4af0cbff",
      "metadata": {
        "id": "0d4c9479-8bfd-4f2b-b4d3-bd4d4af0cbff",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# from llama_hub.file import PDFReader\n",
        "\n",
        "# loader = PDFReader('/content/data/2303.08774.pdf')\n",
        "# documents = loader.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bae82b55-5c9f-432a-9e06-1fccb6f9fc7f",
      "metadata": {
        "id": "bae82b55-5c9f-432a-9e06-1fccb6f9fc7f"
      },
      "source": [
        "#### Parse into Nodes\n",
        "Document stores contain ingested document chunks, which LlamaIndex calls 'Node' objects.\n",
        "\n",
        "\n",
        "By default, the SimpleDocumentStore stores Node objects in-memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f97e558a-c29f-44ec-ab33-1f481da1a6ef",
      "metadata": {
        "id": "f97e558a-c29f-44ec-ab33-1f481da1a6ef",
        "scrolled": true,
        "tags": []
      },
      "outputs": [],
      "source": [
        "from llama_index.node_parser import SimpleNodeParser\n",
        "nodes = SimpleNodeParser().get_nodes_from_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#nodes"
      ],
      "metadata": {
        "id": "KwU10TXHCc92"
      },
      "id": "KwU10TXHCc92",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "7b81e70f",
      "metadata": {
        "id": "7b81e70f"
      },
      "source": [
        "## Persisting nodes and indexes to MongoDB\n",
        "There is an option to persist the nodes as an actual collection in mongoDB using MongoDocumentStore. Here we would persist the data in mongoDB. \n",
        "Storing the ‘LlamaIndex documents’ and indexes in a database becomes necessary in a couple of scenarios:\n",
        "(a) Use cases where large datasets require more than in-memory storage.\n",
        "(b) Ingesting and processing data from various sources (for example, PDFs, google docs, Slack).\n",
        "(c) The requirement to continuously maintain updates from the underlying data sources. \n",
        "\n",
        "Being able to persist this data enables processing the data once and then being able to query it for various downstream applications. You can easily reconnect to your MongoDB collection and reload the index by re-initializing a MongoIndexStore with an existing db_name and collection_name.\n",
        "\n",
        "MongoDB offers a free forever Atlas cluster in the public cloud service of your choice. Quickly create a free forever Atlas cluster by following this [tutorial](https://www.mongodb.com/developer/products/atlas/free-atlas-cluster/). Or you can get started directly [here](https://www.mongodb.com/cloud/atlas/register). \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a6071d9",
      "metadata": {
        "id": "3a6071d9"
      },
      "outputs": [],
      "source": [
        "MONGO_URI = \"MONGO_DB_URI_HERE\"\n",
        "MONGODB_DATABASE = \"DB_NAME_HERE\"\n",
        "# Note: You can configure the db_name and namespace when instantiating MongoDocumentStore & MongoIndexStore, \n",
        "# otherwise they default to db_name=\"db_docstore\" and namespace=\"docstore\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aff4c8e1-b2ba-4ea6-a8df-978c2788fedc",
      "metadata": {
        "id": "aff4c8e1-b2ba-4ea6-a8df-978c2788fedc"
      },
      "source": [
        "#### Add Nodes to MongoDB backed Docstore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d46adf9a",
      "metadata": {
        "id": "d46adf9a"
      },
      "outputs": [],
      "source": [
        "from llama_index.storage.docstore import MongoDocumentStore\n",
        "docstore = MongoDocumentStore.from_uri(uri=MONGO_URI)\n",
        "\n",
        "docstore.add_documents(nodes)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b262e0ce",
      "metadata": {
        "id": "b262e0ce"
      },
      "source": [
        "This would result in a new collection called `docstore/data` and `docstore/metadata` being created in mongoDB"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69ec9c02",
      "metadata": {
        "id": "69ec9c02"
      },
      "source": [
        "![MongoDocumentStore](https://drive.google.com/uc?export=view&id=1PrMet1I8bWfd-6pf4YK8RtQmRYFpLdVu)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5f85f6d",
      "metadata": {
        "id": "a5f85f6d"
      },
      "source": [
        "### Define Indexes & Store them in MongoDB\n",
        "\n",
        "\n",
        "Each index uses the same underlying Docstore."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b61db18",
      "metadata": {
        "id": "1b61db18"
      },
      "outputs": [],
      "source": [
        "from llama_index.storage.docstore import MongoDocumentStore\n",
        "from llama_index.storage.index_store import MongoIndexStore\n",
        "from llama_index.storage.storage_context import StorageContext\n",
        "\n",
        "storage_context = StorageContext.from_defaults(\n",
        "    docstore=MongoDocumentStore.from_uri(uri=MONGO_URI, db_name=MONGODB_DATABASE),\n",
        "    index_store=MongoIndexStore.from_uri(uri=MONGO_URI, db_name=MONGODB_DATABASE),\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "409d5d13",
      "metadata": {
        "id": "409d5d13"
      },
      "outputs": [],
      "source": [
        "list_index = GPTListIndex(nodes, storage_context=storage_context)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2f4c6be",
      "metadata": {
        "id": "c2f4c6be"
      },
      "outputs": [],
      "source": [
        "vector_index = GPTVectorStoreIndex(nodes, storage_context=storage_context) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4d8ef30",
      "metadata": {
        "id": "c4d8ef30"
      },
      "outputs": [],
      "source": [
        "keyword_table_index = GPTSimpleKeywordTableIndex(nodes, storage_context=storage_context) "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dgqVlJrd-ajJ",
      "metadata": {
        "id": "dgqVlJrd-ajJ"
      },
      "source": [
        "This would result in a new collection called `index_store/data` being created in mongoDB"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1b2293a",
      "metadata": {
        "id": "c1b2293a"
      },
      "source": [
        "![MongoIndexStore](https://drive.google.com/uc?export=view&id=1JkpyWyJjXLLC-0i1Q2NCflDG5RyDUQbk)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "948a5b10",
      "metadata": {
        "id": "948a5b10"
      },
      "source": [
        "### Retrieve Nodes from MongoDB Docstore\n",
        "\n",
        "(This is an OPTIONAL step. If you have been following along till now, the documents are already loaded in-memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ba8b0da-67a8-4653-8cdb-09e39583a2d8",
      "metadata": {
        "id": "1ba8b0da-67a8-4653-8cdb-09e39583a2d8",
        "scrolled": true,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e49228c0-8bf2-4919-d93d-1bd75caf82c4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "from llama_index.storage.docstore import MongoDocumentStore\n",
        "docstore = MongoDocumentStore.from_uri(uri=MONGO_URI, db_name=MONGODB_DATABASE)\n",
        "nodes = list(docstore.docs.values())\n",
        "\n",
        "# NOTE: Verify that the docstore still has the same nodes\n",
        "len(docstore.docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3bf6aaf-3375-4212-8323-777969a918f7",
      "metadata": {
        "id": "d3bf6aaf-3375-4212-8323-777969a918f7"
      },
      "source": [
        "## Test out some Queries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "036077b7-108e-4026-9628-44c694343460",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        },
        "id": "036077b7-108e-4026-9628-44c694343460",
        "outputId": "a989eee9-02f6-4abd-ebce-cf30120c9f34",
        "scrolled": true,
        "tags": []
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**`Final Response:`** Yes, he does have experience with Salesforce. He mentions that he \"built CRM systems by building reports, dashboards, automation, and integrations to improve internal processes\" while working as a Product Owner II at ClearForMe. He also mentions that he \"designed and developed reports and dashboards by understanding customer need in Salesforce\" while working as a Technical Business Analyst at Cloud Mentor."
          },
          "metadata": {}
        }
      ],
      "source": [
        "vector_response = vector_index.as_query_engine().query(\"Does he have experience with Salesforce?\") \n",
        "display_response(vector_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff58018c-3117-4d50-abff-16a1873eda9c",
      "metadata": {
        "id": "ff58018c-3117-4d50-abff-16a1873eda9c",
        "scrolled": true,
        "outputId": "929f8311-4669-4395-c760-fe64af94da19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**`Final Response:`** The companies Ananth Prayaga worked at are:\n1. Slyce\n2. Independence Blue Cross\n3. Temple University\n4. ClearForMe\n5. Cloud Mentor\n6. Comcast"
          },
          "metadata": {}
        }
      ],
      "source": [
        "vector_response = vector_index.as_query_engine().query(\"What are all the companies he worked at?\") \n",
        "display_response(vector_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aeb8e427",
      "metadata": {
        "id": "aeb8e427",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 47
        },
        "outputId": "6c59a554-86ea-43b7-cb0c-744190bdeec2"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**`Final Response:`** His name is Ananth Prayaga."
          },
          "metadata": {}
        }
      ],
      "source": [
        "vector_response = vector_index.as_query_engine().query(\"What is his name?\") \n",
        "display_response(vector_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f2f8fc0",
      "metadata": {
        "id": "3f2f8fc0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "a292070d-0edb-4909-f92e-b29c056ea86e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**`Final Response:`** Migration?\n\nYes, Ananth Prayaga has experience with data migration. He has experience extracting data/files from various legacy systems, transforming data for loading into source systems, and architecting the data migration process for a sunsetting legacy library system."
          },
          "metadata": {}
        }
      ],
      "source": [
        "vector_response = vector_index.as_query_engine().query(\"Does he have experience with Data\") \n",
        "display_response(vector_response)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vector_response = vector_index.as_query_engine().query(\"List all his skills?\") \n",
        "display_response(vector_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZajOPSKPFYeI",
        "outputId": "cd83ab50-c022-4d47-c571-7927488bdfce"
      },
      "id": "ZajOPSKPFYeI",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**`Final Response:`** - Data Analytics\n- Business Analysis\n- Product Management\n- Data Migration\n- Reporting\n- Analytics\n- SaaS Product Development\n- Cross-Functional Team Management\n- Google Data Studio\n- BigQuery\n- JIRA\n- AirTable\n- Excel\n- Machine Learning\n- Sales Analysis\n- Focus Groups\n- Interviews\n- Surveys\n- Desk Research\n- Primary Research\n- Secondary Research\n- Google Data Studio\n- BigQuery\n- JIRA\n- AirTable\n- Excel\n- Machine Learning\n- Sales Analysis\n- Focus Groups\n- Interviews\n- Surveys\n- Desk Research\n- Primary Research\n- Secondary Research\n- ETL Jobs\n- Projects\n- Job Schedules\n- Custom Reports\n- IBM Cognos\n- Text Analytics\n- Product Strategy\n- Roadmap Development\n- CRM Systems\n- Reports\n- Dashboards\n- Automation\n- Integrations\n- API Product Offering\n- Core Platform Development\n- Product Operations\n- Launch Support\n- Data Extraction\n- Data Mapping\n- SQL\n- ETL Tools\n- Report and Dashboard Development\n- Python Scripting\n- Accounting Data Consolidation"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hg8-zkXQIQe7"
      },
      "id": "Hg8-zkXQIQe7",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}